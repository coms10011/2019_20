%9_central_limit_theorem
%notes for the course Probability and Statistics COMS10011
%taught at the University of Bristol
%Conor Houghton conor.houghton@bristol.ac.uk
%To the extent possible under law, the author has dedicated all copyright
%and related and neighboring rights to these notes to the public domain 
%worldwide. These notes are distributed without any warranty. 

\documentclass[11pt,a4paper]{scrartcl}
\typearea{12}
\usepackage{graphicx}
%\usepackage{pstricks}
\usepackage{listings}
\usepackage{color}

\newif\ifind
\indtrue

\lstset{language=C}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lfoot{\texttt{coms10011.github.io}}
\lhead{COMS100011 9\_central\_limit\_theorem - Conor}
\begin{document}

\section*{9 The Central Limit Theorem}

\subsection*{The sum of two random variables}

Say we have two continuous random variables $X$ and $Y$, with density
functions $p_X(x)$ and $p_Y(y)$; consider working out the distribution for 
\begin{equation}
Z=X+Y
\end{equation}
Now the thing is that there is more than one value for $x$ and $y$
that gives $Z=z$. In fact $Z=z$ if $Y=z-x$ for any value of $x$ so
\begin{equation}
p_Z(z)=\int_{-\infty}^\infty {p_X(x)p_Y(z-x)dx}
\end{equation}
This calculation is called a \textbf{convolution}.

As a simple example consider $X$ and $Y$ both uniformly distributed
over $[0,1]$:
\begin{equation}
p_X(x)=\left\{\begin{array}{ll}1:x\in[0,1]\\0:\mbox{otherwise}\end{array}\right.
\end{equation}
with $p_Y(y)$ the same. Now $Z=X+Y$ has the distribution
\begin{equation}
p_Z(z)=\int_{-\infty}^\infty {p_X(x)p_Y(z-x)dx}
\end{equation}
where the integrand is zero unless $x$ and $z-x$ are both inside the
interval zero to one. Now, the first contraint, $x\in [0,1]$ is easy
but the second one depends on $z$. If $z<0$ then $z-x$ is always
negative and $p_Y(z-x)=0$. If $z>2$ then $z-x>1$ and again
$p_Y(z-x)=0$. If $z\in [0,2]$ there are two cases, if $z<1$ then $x$
then $z-x>0$ for $x\in [0,z]$, otherwise if $z>1$ then $z-x<1$ in $x\in
[z-1,1]$. Lets do the two cases separately, for $z\in [0,1]$:
\begin{equation}
p_Z(z)=\int_0^z dx=z
\end{equation}
and for $z\in (1,2]$
\begin{equation}
p_Z(z)=\int_{z-1}^1dx=1-(z-1)=2-z
\end{equation}
Putting it all together we get
\begin{equation}
p_Z(z)=\left\{\begin{array}{ll} z&z\in [0,1]\\2-z& z\in [1,2]\\0&\mbox{otherwise}\end{array}\right.
\end{equation}

\subsection*{The sum of two Gau\ss{}ians}

Next lets consider the sum of two Gau\ss{}ians, $X\sim
\mathcal{N}(\mu_x,\sigma_x^2)$ and $Y\sim
\mathcal{N}(\mu_Y,\sigma_Y^2)$. Calculating the distribution of $Z=X+Y$ would be difficult since it involves the integral
\begin{equation}
p_Z(z)=\frac{1}{2\pi\sigma_X^2\sigma_Y^2}
\int_{-\infty}^\infty e^{-(x-\mu_X)^2/2\sigma_X^2-(z-x-\mu_Y)^2/2\sigma_Y^2}dx
\end{equation}
We aren't going to attempt that, but we will quote the result: the sum is also a Gau\ss{}ian:
\begin{equation}
Z\sim \mathcal{N}(\mu_X+\mu_Y,\sigma_X^2+\sigma_Y^2)
\end{equation}

\subsection*{The central limit theorem}

Let $\{X_1,X_2,\ldots,X_n\}$ be a set of random variables. A set of
random variables is called \textbf{independent identically distributed},
usually abbreviated to i.i.d., if the variables all have the same
probability density, say $p_X(x)$, and if the variables are independent. Let $\mu$ and
$\sigma^2$ be the mean and variance of that probability density. Now
consider the \textbf{sample mean}:
\begin{equation}
S_n=\frac{X_1+X_2+\ldots+X_n}{n}
\end{equation}
If you were estimating the mean for $X$ this is the value you would
measure if you took $n$ independent samples. Now the central limit
theorem states that for $n$ approaching infinity
\begin{equation}
U_n=\sqrt{n}\left(\frac{S_n-\mu}{\sigma}\right) \sim \mathcal{N}(0,1)
\end{equation}

As an example, consider the uniform distribution example we looked
before; in Fig.~\ref{fig_const} we plot $U_n$ for different $n$. We
see that it very quickly comes to resemble a Gau\ss{}ian distribution.

\begin{figure}[tb]
\begin{center}
\include{conv}
\end{center}
\caption{$U_n$ for different $n$s where $X$ is the uniform distribution.\label{fig_const}}
\end{figure}

We won't prove the central limit theorem here, doing so involves a bit
of detail in deciding what we mean by \lq{}for $n$ approaching
infinity\rq{}. The proof itself involves some messing about with the
moment generating theorem which we haven't studied, but otherwise
isn't too hard. It isn't particularly revealing though; it misses what
is happening, which is roughly this: the central limit theorem is a
bit like the binomial distribution, just like the binomial
distribution quantifies the different ways to get different numbers of
$S$, the central limit theorem quantifies different ways of getting
the same sum. In fact, numerically, the Gau\ss{}ian distribution is
close to the binomial distribution, see for example
Fig.~\ref{fig_binomial}.


\begin{figure}[tb]
\begin{center}
\include{binomial}
\end{center}
\caption{This compares the binomial distribution with $n=30$ and
  $p=0.25$ with the  Gau\ss{}ian distribution with the same mean and variance.\label{fig_binomial}}
\end{figure}


The central limit theorem is important in two ways. First off, it
tells us how the sample mean behaves; since we often estimate means by
doing multiple independent trials this is useful. The second point is
vaguer, although we have stated the central limit theorem in terms of
i.i.d. variables and this is the case where it is easy to state and
prove the theorem, there is a more general phenomena, which is that if
you add independent variables, even if they are not identical, then
the result is a Gau\ss{}ian. This is why so many things show
Gau\ss{}ian distributions. Think, for example, of the height of trees,
lots of things contribute to the height of a tree, how wet and sunny
the location it is growing in, the soil, the genetics of each
individual. These individual contributions probably aren't
Gau\ss{}ian, they aren't identical and, in fact, probably don't
contribute in a linear way to the height, nonetheless we would expect
the result, the height of the trees to be Gau\ss{}ian. It is a
fortunate aspect of statistics, things that are complicated and have
multiple contributing factors, tend to behave in a simple way from the
point-of-view of statistics.

\include{summary}
\include{example_question}

\end{document}




